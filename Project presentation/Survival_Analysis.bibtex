@article{PCR1,
author = {Hotelling, Harold},
title = {THE RELATIONS OF THE NEWER MULTIVARIATE STATISTICAL METHODS TO FACTOR ANALYSIS},
journal = {British Journal of Statistical Psychology},
volume = {10},
number = {2},
pages = {69-79},
doi = {https://doi.org/10.1111/j.2044-8317.1957.tb00179.x},
url = {https://bpspsychub.onlinelibrary.wiley.com/doi/abs/10.1111/j.2044-8317.1957.tb00179.x},
eprint = {https://bpspsychub.onlinelibrary.wiley.com/doi/pdf/10.1111/j.2044-8317.1957.tb00179.x},
abstract = {Abstract. A survey of developments in multivariate analysis during the last thirty years shows that some, though not all, of the purposes for which factor analysis has been used may now be better accomplished by other procedures, e.g. by regression, multiple correlation, the study of relations between two sets of variates, or of the dimensionality of a set of variates. To determine whether two or more groups of persons differ significantly in their mean values or their covariance matrices, the most appropriate procedures consist of methods of multivariate analysis of variance. Such methods have increased rather than diminished the advantages of using an external criterion instead of making a purely internal analysis. The problem of estimating the dimensionality of a continuous multivariate population by means of a sample admits a simple and exact answer: the rank of the population is equal to that of the sample, provided the number of degrees of freedom among the individuals in the sample exceeds the number of the variates. If, however, the problem is re-interpreted so as to imply that each observation is the sum of a real part and a random error, then the hypothesis that the real part has a rank smaller than the number of variates can be tested only when a proper estimate of the errors is available, i.e. by making suitable replications. The appropriate method is again multivariate analysis of variance rather than factor analysis. In a third form of the problem the object is, not to ascertain a true dimensionality, but to assess the error resulting from discarding (on practical grounds) a dimension that may really exist. Here the problem is somewhat indeterminate: a practical expedient may involve a special form of factor analysis, viz. the calculation of the least, not the greatest, latent roots. Thus, in many cases in which they have hitherto been used, factor analyses of the usual kinds are inferior to other procedures: nevertheless, the results of such analyses may have heuristic and suggestive value, and may uncover hypotheses which are capable of more objective testing by other methods.},
year = {1957}
}

@article{PCR2,
  title={Course in multivariate analysis},
  author={Kendall, Maurice G and others},
  year={1957},
  publisher={Charles Griffin \& Co.}
}

@article{l0,
    author = {Donoho, David L and Johnstone, Iain M},
    title = "{Ideal spatial adaptation by wavelet shrinkage}",
    journal = {Biometrika},
    volume = {81},
    number = {3},
    pages = {425-455},
    year = {1994},
    month = {09},
    abstract = "{With ideal spatial adaptation, an oracle furnishes information about how best to adapt a spatially variable estimator, whether piecewise constant, piecewise polynomial, variable knot spline, or variable bandwidth kernel, to the unknown function. Estimation with the aid of an oracle offers dramatic advantages over traditional linear estimation by nonadaptive kernels; however, it is a priori unclear whether such performance can be obtained by a procedure relying on the data alone. We describe a new principle for spatially-adaptive estimation: selective wavelet reconstruction. We show that variable-knot spline fits and piecewise-polynomial fits, when equipped with an oracle to select the knots, are not dramatically more powerful than selective wavelet reconstruction with an oracle. We develop a practical spatially adaptive method, Risk Shrink, which works by shrinkage of empirical wavelet coefficients. RiskShrink mimics the performance of an oracle for selective wavelet reconstruction as well as it is possible to do so. A new inequality in multivariate normal decision theory which we call the oracle inequality shows that attained performance differs from ideal performance by at most a factor of approximately 2 log n, where n is the sample size. Moreover no estimator can give a better guarantee than this. Within the class of spatially adaptive procedures, RiskShrink is essentially optimal. Relying only on the data, it comes within a factor log2n of the performance of piecewise polynomial and variableknot spline methods equipped with an oracle. In contrast, it is unknown how or if piecewise polynomial methods could be made to function this well when denied access to an oracle and forced to rely on data alone.}",
    issn = {0006-3444},
    doi = {10.1093/biomet/81.3.425},
    url = {https://doi.org/10.1093/biomet/81.3.425},
    eprint = {https://academic.oup.com/biomet/article-pdf/81/3/425/26079146/81.3.425.pdf},
}

@article{l1,
 ISSN = {00359246},
 URL = {http://www.jstor.org/stable/2346178},
 abstract = {We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
 author = {Robert Tibshirani},
 journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
 number = {1},
 pages = {267--288},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Regression Shrinkage and Selection via the Lasso},
 urldate = {2022-11-27},
 volume = {58},
 year = {1996}
}

@article{l2,
 ISSN = {00401706},
 URL = {http://www.jstor.org/stable/1271436},
 abstract = {In multiple regression it is shown that parameter estimates based on minimum residual sum of squares have a high probability of being unsatisfactory, if not incorrect, if the prediction vectors are not orthogonal. Proposed is an estimation procedure based on adding small positive quantities to the diagonal of X′X. Introduced is the ridge trace, a method for showing in two dimensions the effects of nonorthogonality. It is then shown how to augment X′X to obtain biased estimates with smaller mean square error.},
 author = {Arthur E. Hoerl and Robert W. Kennard},
 journal = {Technometrics},
 number = {1},
 pages = {80--86},
 publisher = {[Taylor & Francis, Ltd., American Statistical Association, American Society for Quality]},
 title = {Ridge Regression: Biased Estimation for Nonorthogonal Problems},
 urldate = {2022-11-27},
 volume = {42},
 year = {2000}
}

@article{Cox,
 ISSN = {00359246},
 URL = {http://www.jstor.org/stable/2985181},
 abstract = {The analysis of censored failure times is considered. It is assumed that on each individual are available values of one or more explanatory variables. The hazard function (age-specific failure rate) is taken to be a function of the explanatory variables and unknown regression coefficients multiplied by an arbitrary and unknown function of time. A conditional likelihood is obtained, leading to inferences about the unknown regression coefficients. Some generalizations are outlined.},
 author = {D. R. Cox},
 journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
 number = {2},
 pages = {187--220},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Regression Models and Life-Tables},
 urldate = {2022-11-28},
 volume = {34},
 year = {1972}
}
@article{LASSOCOX,
author = {Robert Tibshirani},
title = {THE LASSO METHOD FOR VARIABLE SELECTION IN THE COX MODEL},
journal = {Statistics in Medicine},
volume = {16},
number = {4},
pages = {385-395},
doi = {https://doi.org/10.1002/(SICI)1097-0258(19970228)16:4<385::AID-SIM380>3.0.CO;2-3},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291097-0258%2819970228%2916%3A4%3C385%3A%3AAID-SIM380%3E3.0.CO%3B2-3},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/%28SICI%291097-0258%2819970228%2916%3A4%3C385%3A%3AAID-SIM380%3E3.0.CO%3B2-3},
abstract = {Abstract I propose a new method for variable selection and shrinkage in Cox's proportional hazards model. My proposal minimizes the log partial likelihood subject to the sum of the absolute values of the parameters being bounded by a constant. Because of the nature of this constraint, it shrinks coefficients and produces some coefficients that are exactly zero. As a result it reduces the estimation variance while providing an interpretable final model. The method is a variation of the ‘lasso’ proposal of Tibshirani, designed for the linear regression context. Simulations indicate that the lasso can be more accurate than stepwise selection in this setting. © 1997 by John Wiley \& Sons, Ltd.},
year = {1997}
}
@article{EnetCox,
 ISSN = {13697412, 14679868},
 URL = {http://www.jstor.org/stable/4623289},
 abstract = {We introduce a path following algorithm for L_{1}$-regularized generalized linear models. The L_{1}$-regularization procedure is useful especially because it, in effect, selects variables according to the amount of penalization on the L_{1}$-norm of the coefficients, in a manner that is less greedy than forward selection-backward deletion. The generalized linear model path algorithm efficiently computes solutions along the entire regularization path by using the predictor-corrector method of convex optimization. Selecting the step length of the regularization parameter is critical in controlling the overall accuracy of the paths; we suggest intuitive and flexible strategies for choosing appropriate values. We demonstrate the implementation with several simulated and real data sets.},
 author = {Mee Young Park and Trevor Hastie},
 journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
 number = {4},
 pages = {659--677},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {L1-Regularization Path Algorithm for Generalized Linear Models},
 urldate = {2022-11-28},
 volume = {69},
 year = {2007}
}

@article{cindex,
author = {Harrell Jr., FRANK E. and Lee, Kerry L. and MARK, DANIEL B.},
title = {MULTIVARIABLE PROGNOSTIC MODELS: ISSUES IN DEVELOPING MODELS, EVALUATING ASSUMPTIONS AND ADEQUACY, AND MEASURING AND REDUCING ERRORS},
journal = {Statistics in Medicine},
volume = {15},
number = {4},
pages = {361-387},
doi = {https://doi.org/10.1002/(SICI)1097-0258(19960229)15:4<361::AID-SIM168>3.0.CO;2-4},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291097-0258%2819960229%2915%3A4%3C361%3A%3AAID-SIM168%3E3.0.CO%3B2-4},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/%28SICI%291097-0258%2819960229%2915%3A4%3C361%3A%3AAID-SIM168%3E3.0.CO%3B2-4},
abstract = {Abstract Multivariable regression models are powerful tools that are used frequently in studies of clinical outcomes. These models can use a mixture of categorical and continuous variables and can handle partially observed (censored) responses. However, uncritical application of modelling techniques can result in models that poorly fit the dataset at hand, or, even more likely, inaccurately predict outcomes on new subjects. One must know how to measure qualities of a model's fit in order to avoid poorly fitted or overfitted models. Measurement of predictive accuracy can be difficult for survival time data in the presence of censoring. We discuss an easily interpretable index of predictive discrimination as well as methods for assessing calibration of predicted survival probabilities. Both types of predictive accuracy should be unbiasedly validated using bootstrapping or cross-validation, before using predictions in a new data series. We discuss some of the hazards of poorly fitted and overfitted regression models and present one modelling strategy that avoids many of the problems discussed. The methods described are applicable to all regression models, but are particularly needed for binary, ordinal, and time-to-event outcomes. Methods are illustrated with a survival analysis in prostate cancer using Cox regression.},
year = {1996}
}

